---
layout: post
title:  "Recurrent Experience Replay in Distributed Reinforcement Learning 논문 리뷰 및 설명"
subtitle:   ""
categories: deeplearning
tags: reinforcementlearning
---

# Recurrent Experience Replay in Distributed Reinforcement Learning

## Abstract
이 논문은 최근 distributed RL의 성공적인 성능의 기반하여 RNN 계열의 Ape-X(이전에 리뷰한 논문에 있습니다.)에 대해 조사한 논문입니다.

we study the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved strategy.

## introduction

RL은 최근 여러 도전적인 문제들을 해결하면서 재조명을 받았습니다. 이러한 성공에는 초기에는 experience replay와 frame을 stack해서 input으로 넣는 technic들이 있습니다. 하지만 문제들이 점점 어려워지면서, memory기반의 representation의 요구가 증가했고 결국에, RNN계열의 layer를 적용시켜 이를 효과적으로 해결하였습니다.

이 논문에서는 RNN을 experience replay와함께 학습하는 것에 대해 조사했고, 이에 대해 세가지 contribution을 남겼습니다. 첫째로, experience replay가 parameter lag(representational drift와 recurrent state staleness를 이끄는)에 얼마나 영향을 미치는지 설명합니다. 이는 distributed RL에서 안정성과 성능을 떨어뜨립니다. 둘째로, experience replay를 사용한 RNN training을 하면서 이전의 효과들을 완화하는 것을 보입니다. 셋째로, 위의 말한 것들을 통합해 좋은 결과를내는 것을 보입니다.

## Background

### 2.3 The Recurrent Replay Distributed DQN Agent

이 논문에서는 Recurrent Replay Distributed DQN(R2D2)라는 새로운 agent를 소개합니다. recurrent
