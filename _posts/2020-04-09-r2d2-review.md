---
layout: post
title:  "Recurrent Experience Replay in Distributed Reinforcement Learning 논문 리뷰 및 설명"
subtitle:   ""
categories: deeplearning
tags: reinforcementlearning
---

# Recurrent Experience Replay in Distributed Reinforcement Learning

## Abstract
이 논문은 최근 distributed RL의 성공적인 성능의 기반하여 RNN 계열의 Ape-X(이전에 리뷰한 논문에 있습니다.)에 대해 조사한 논문입니다.

we study the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved strategy.

## introduction

RL은 최근 여러 도전적인 문제들을 해결하면서 재조명을 받았습니다. 이러한 성공에는 초기에는 experience replay와 frame을 stack해서 input으로 넣는 technic들이 있습니다. 하지만 문제들이 점점 어려워지면서, memory기반의 representation의 요구가 증가했고 결국에, RNN계열의 layer를 적용시켜 이를 효과적으로 해결하였습니다.

이 논문에서는 RNN을 experience replay와함께 학습하는 것에 대해 조사했고, 이에 대해 세가지 contribution을 남겼습니다. 첫째로, experience replay가 parameter lag(representational drift와 recurrent state staleness를 이끄는)에 얼마나 영향을 미치는지 설명합니다. 이는 distributed RL에서 안정성과 성능을 떨어뜨립니다. 둘째로, experience replay를 사용한 RNN training을 하면서 이전의 효과들을 완화하는 것을 보입니다. 셋째로, 위의 말한 것들을 통합해 좋은 결과를내는 것을 보입니다.

## Background
### 2.2 Distributed Reinforcement learning
 Impala는 transition의 sequence와 함께, initial recurrent state를 experience queue에 넣는 방식을 사용합니다. 


### 2.3 The Recurrent Replay Distributed DQN Agent

이 논문에서는 Recurrent Replay Distributed DQN(R2D2)라는 새로운 agent를 소개합니다. 이를 사용해 recurrent state와 experience replay, 그리고 distributed training의 사이에서 상호작용을 하는 것에 대해 연구하였습니다. r2d2는 ape-X처럼 PER과 n-step double Q-learning, 많은 actor로 부터 오는 replay를 단일 learner가 학습한다는 점, dueling network를 사용한다는 점에서 유사한점이 많습니다. 하지만, 여기서는 Convolutional layer뒤에 LSTM layer를 통해 학습하는 점이 크게 다른점입니다.

그렇게 하기 위해서, 여기서는 일반적인 sars'의 transition tuple의 data 구조 대신, 인접한 sequence는 40 step씩 서로 겹치게, 정해진 길이(m=80)의 sar를 저장하였습니다. (물론 episode boundary는 넘지 않게)
학습할 때에는, 같은 길이의 states를 가지고, network와 target network에 input으로 넣어주었습니다.

